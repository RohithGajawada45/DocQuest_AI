# PDF Query App

A full‑stack web application that lets you upload a PDF, index its contents into a Chroma vector store, and then ask natural‑language questions against it using a Retrieval‑Augmented Generation (RAG) pipeline powered by a local Ollama LLM.

## Features

- Drag & drop PDF upload UI (React + Vite)
- Automatic PDF chunking & embedding (LangChain + HuggingFace)
- Persistent Chroma vector store for fast similarity search
- RAG prompt assembly and local LLM inference (Ollama, Mistral model)
- Toast notifications for workflow feedback

## Tech Stack

- **Frontend**: React + TypeScript, Vite bundler, Tailwind CSS, react‑dropzone, react‑hot‑toast, axios
- **Backend**: Python, Flask, flask‑cors, werkzeug
- **Vector Embeddings**: LangChain + `sentence‑transformers/all‑MiniLM‑L6‑v2` via `langchain‑huggingface`
- **Vector Store**: ChromaDB (persistent on disk)
- **LLM**: Ollama (Mistral model)
- **PDF Processing**: `PyPDFLoader` (langchain‑community), `pypdf`

## Prerequisites

1. **Node.js** v16+ and **npm** or **yarn**
2. **Python** 3.8+ and **pip**
3. **Ollama CLI** installed locally

## Ollama Installation & Setup

1. Install the Ollama CLI:
   - macOS (Homebrew):
     ```bash
     brew tap ollama/tap
     brew install ollama
     ```
   - Linux (curl script):
     ```bash
     curl -sSL https://ollama.com/install.sh | sh
     ```
2. Verify installation:
   ```bash
   ollama --version
   ```
3. Pull the Mistral model:
   ```bash
   ollama pull mistral
   ```
4. (Optional) Test the model:
   ```bash
   ollama run mistral "Hello, Ollama!"
   ```

## Installation

### 1. Clone the Repository

```bash
git clone <your-repo-url>
cd DocQuest_AI
```

### 2. Backend Setup (Flask)

```bash
cd src
python3 -m venv .venv                # create a virtual environment
source .venv/bin/activate            # activate it
pip install --upgrade pip
pip install -r requirements.txt      # install Python dependencies
```

#### Configuration

- By default, the vector store lives in `src/chroma_db` and uploads in `src/uploads`.
- **PORT** environment variable can override the Flask port (defaults to 5000):
  ```bash
  PORT=5001 python app.py
  ```

### 3. Frontend Setup (React/Vite)

Open a new terminal at the project root:

```bash
npm install      # or yarn install
```

#### Configuration

You can configure the API base URL with a Vite env var. Create a file `.env` in the project root with:

```dotenv
VITE_API_URL=http://localhost:5000
```

Vite will automatically pick up `VITE_API_URL` at build time.

## Running the App

1. **Start the Backend** (in `src/`):
   ```bash
   source .venv/bin/activate  # if not already active
   python app.py
   ```
   The Flask API will be accessible at `http://localhost:5000` (or your chosen `PORT`).

2. **Start the Frontend** (project root):
   ```bash
   npm run dev
   ```
   Open your browser at `http://localhost:5173`.

3. **Use the App**:
   - Upload a PDF. The server will clear previous uploads, re‑index the document, and store embeddings in Chroma.
   - Ask questions in the "Ask Questions" panel; answers are generated by querying the vector store and using the Ollama LLM for RAG.

## Troubleshooting

- **Port already in use**: kill the process or set a different `PORT` for Flask.
- **Ollama errors**: ensure the CLI is installed and the `mistral` model is pulled.
- **CORS issues**: check that `flask-cors` is installed and active in `app.py`.

## License

This project is MIT‑licensed. Feel free to fork and adapt! 
